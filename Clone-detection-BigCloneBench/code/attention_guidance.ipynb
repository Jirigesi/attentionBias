{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "from transformers import  RobertaConfig, RobertaModel, RobertaTokenizer\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from model2 import Model\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "import random\n",
    "import multiprocessing\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "import javalang\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "np.random.seed(0)\n",
    "import seaborn as sns\n",
    "import collections\n",
    "import pickle\n",
    "import sklearn\n",
    "from matplotlib import cm\n",
    "from sklearn import manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, sample_size=1000, file_path='train', block_size=512,pool=None):\n",
    "        postfix=file_path.split('/')[-1].split('.txt')[0]\n",
    "        self.examples = []\n",
    "        index_filename=file_path\n",
    "        print(\"Creating features from index file at %s \", index_filename)\n",
    "        url_to_code={}\n",
    "        with open('/'.join(index_filename.split('/')[:-1])+'/data.jsonl') as f:\n",
    "            for line in f:\n",
    "                line=line.strip()\n",
    "                js=json.loads(line)\n",
    "                url_to_code[js['idx']]=js['func']\n",
    "        data=[]\n",
    "        cache={}\n",
    "        f=open(index_filename)\n",
    "        with open(index_filename) as f:\n",
    "            for line in f:\n",
    "                line=line.strip()\n",
    "                url1,url2,label=line.split('\\t')\n",
    "                if url1 not in url_to_code or url2 not in url_to_code:\n",
    "                    continue\n",
    "                if label=='0':\n",
    "                    label=0\n",
    "                else:\n",
    "                    label=1\n",
    "                data.append((url1,url2,label,tokenizer,cache,url_to_code))\n",
    "        data=data[:sample_size]\n",
    "\n",
    "        self.examples=pool.map(get_example,tqdm(data,total=len(data)))\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item].input_ids),torch.tensor(self.examples[item].label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example(item):\n",
    "    url1,url2,label,tokenizer,cache,url_to_code=item\n",
    "    if url1 in cache:\n",
    "        code1=cache[url1].copy()\n",
    "    else:\n",
    "        try:\n",
    "            code=' '.join(url_to_code[url1].split())\n",
    "        except:\n",
    "            code=\"\"\n",
    "        code1=tokenizer.tokenize(code)\n",
    "    if url2 in cache:\n",
    "        code2=cache[url2].copy()\n",
    "    else:\n",
    "        try:\n",
    "            code=' '.join(url_to_code[url2].split())\n",
    "        except:\n",
    "            code=\"\"\n",
    "        code2=tokenizer.tokenize(code)\n",
    "        \n",
    "    return convert_examples_to_features(code1,code2,label,url1,url2,tokenizer,block_size, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single training/test features for a example.\"\"\"\n",
    "    def __init__(self,\n",
    "                 input_tokens,\n",
    "                 input_ids,\n",
    "                 label,\n",
    "                 url1,\n",
    "                 url2):\n",
    "        self.input_tokens = input_tokens\n",
    "        self.input_ids = input_ids\n",
    "        self.label=label\n",
    "        self.url1=url1\n",
    "        self.url2=url2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(tokenizer, \n",
    "                            test_data_file,\n",
    "                            block_size, \n",
    "                            sample_size=1000,\n",
    "                            evaluate=False,\n",
    "                            test=False,\n",
    "                            pool=None):\n",
    "    dataset = TextDataset(tokenizer, \n",
    "                          file_path=test_data_file,\n",
    "                          sample_size=sample_size,\n",
    "                          block_size=block_size,\n",
    "                          pool=pool)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(code1_tokens,code2_tokens,label,url1,url2,tokenizer,block_size,cache):\n",
    "    code1_tokens=code1_tokens[:block_size-2]\n",
    "    code1_tokens =[tokenizer.cls_token]+code1_tokens+[tokenizer.sep_token]\n",
    "    code2_tokens=code2_tokens[:block_size-2]\n",
    "    code2_tokens =[tokenizer.cls_token]+code2_tokens+[tokenizer.sep_token]  \n",
    "    \n",
    "    code1_ids=tokenizer.convert_tokens_to_ids(code1_tokens)\n",
    "    padding_length = block_size - len(code1_ids)\n",
    "    code1_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "    \n",
    "    code2_ids=tokenizer.convert_tokens_to_ids(code2_tokens)\n",
    "    padding_length = block_size - len(code2_ids)\n",
    "    code2_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "    \n",
    "    source_tokens=code1_tokens+code2_tokens\n",
    "    source_ids=code1_ids+code2_ids\n",
    "    return InputFeatures(source_tokens,source_ids,label,url1,url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = RobertaConfig.from_pretrained('microsoft/codebert-base')\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "model = RobertaModel.from_pretrained('microsoft/codebert-base',\n",
    "                                    output_attentions=True, \n",
    "                                    output_hidden_states=True)\n",
    "\n",
    "model=Model(model,config,tokenizer)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../dataset/valid.txt\"\n",
    "postfix=file_path.split('/')[-1].split('.txt')[0]\n",
    "index_filename=file_path\n",
    "url_to_code={}\n",
    "with open('/'.join(index_filename.split('/')[:-1])+'/data.jsonl') as f:\n",
    "    for line in f:\n",
    "        line=line.strip()\n",
    "        js=json.loads(line)\n",
    "        url_to_code[js['idx']]=js['func']\n",
    "data=[]\n",
    "cache={}\n",
    "f=open(index_filename)\n",
    "with open(index_filename) as f:\n",
    "    # lines = 1000\n",
    "    added_lines = 0\n",
    "    for line in f:\n",
    "        # if added_lines >= lines:\n",
    "        #     break\n",
    "        line=line.strip()\n",
    "        url1,url2,label=line.split('\\t')\n",
    "        if url1 not in url_to_code or url2 not in url_to_code:\n",
    "            continue\n",
    "        if label=='0':\n",
    "            label=0\n",
    "        else:\n",
    "            label=1\n",
    "        data.append((url1,url2,label,' '.join(url_to_code[url1].split()), ' '.join(url_to_code[url2].split())))\n",
    "        added_lines += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "415416"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('13653451',\n",
       " '21955002',\n",
       " 0,\n",
       " 'public ViewInitListener() throws IOException { URL url = this.getClass().getResource(VIEW_INIT_CONFIG); log.debug(\"Loading configuration from: \" + url); config = new Properties(); InputStream in = url.openStream(); config.load(in); in.close(); }',\n",
       " 'public void run() { String s, s2; s = \"\"; s2 = \"\"; try { URL url = new URL(\"http://www.m-w.com/dictionary/\" + Word); BufferedReader in = new BufferedReader(new InputStreamReader(url.openStream())); String str; while (((str = in.readLine()) != null) && (!stopped)) { s = s + str; } in.close(); } catch (MalformedURLException e) { } catch (IOException e) { } Pattern pattern = Pattern.compile(\"popWin\\\\\\\\(\\'/cgi-bin/(.+?)\\'\", Pattern.CASE_INSENSITIVE | Pattern.DOTALL); Matcher matcher = pattern.matcher(s); if ((!stopped) && (matcher.find())) { String newurl = \"http://m-w.com/cgi-bin/\" + matcher.group(1); try { URL url2 = new URL(newurl); BufferedReader in2 = new BufferedReader(new InputStreamReader(url2.openStream())); String str; while (((str = in2.readLine()) != null) && (!stopped)) { s2 = s2 + str; } in2.close(); } catch (MalformedURLException e) { } catch (IOException e) { } Pattern pattern2 = Pattern.compile(\"<A HREF=\\\\\"http://(.+?)\\\\\">Click here to listen with your default audio player\", Pattern.CASE_INSENSITIVE | Pattern.DOTALL); Matcher matcher2 = pattern2.matcher(s2); if ((!stopped) && (matcher2.find())) { if (getWave(\"http://\" + matcher2.group(1))) label.setEnabled(true); } } button.setEnabled(true); }')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syntax_types_for_code(code_snippet):\n",
    "  types = [\"[CLS]\"]\n",
    "  code = [\"<s>\"]\n",
    "  tree = list(javalang.tokenizer.tokenize(code_snippet))\n",
    "  \n",
    "  for i in tree:\n",
    "    j = str(i)\n",
    "    j = j.split(\" \")\n",
    "    if j[1] == '\"MASK\"':\n",
    "      types.append('[MASK]')\n",
    "      code.append('<mask>')\n",
    "    else:\n",
    "      types.append(j[0].lower())\n",
    "      code.append(j[1][1:-1])\n",
    "    \n",
    "  types.append(\"[SEP]\")\n",
    "  code.append(\"</s>\")\n",
    "  return np.array(types), ' '.join(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 400\n",
    "\n",
    "code_sample = data[0]\n",
    "types_1, rewrote_code_1 = get_syntax_types_for_code(code_sample[3])\n",
    "types_2, rewrote_code_2 = get_syntax_types_for_code(code_sample[4])\n",
    "\n",
    "tokenized_ids_1 = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(rewrote_code_1))\n",
    "tokenized_ids_2 = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(rewrote_code_2))\n",
    "if len(tokenized_ids_2) > block_size:\n",
    "    tokenized_ids_2 = tokenized_ids_2[:block_size-1] + [tokenizer.sep_token_id]\n",
    "\n",
    "if len(tokenized_ids_1) > block_size:\n",
    "    tokenized_ids_1 = tokenized_ids_1[:block_size-1] + [tokenizer.sep_token_id]\n",
    "\n",
    "padding_length = block_size - len(tokenized_ids_1)\n",
    "tokenized_ids_1+=[tokenizer.pad_token_id]*padding_length\n",
    "padding_length = block_size - len(tokenized_ids_2)\n",
    "tokenized_ids_2+=[tokenizer.pad_token_id]*padding_length\n",
    "\n",
    "source_ids = tokenized_ids_1 + tokenized_ids_2\n",
    "labels = code_sample[2]\n",
    "source_ids = torch.tensor(source_ids).unsqueeze(0).to(device)\n",
    "labels = torch.tensor(labels).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(block_size,source_ids,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pos_attn_patterns(attentions):\n",
    "    '''\n",
    "    Creates attention patterns related to positional encoding for attention initialization\n",
    "    one_to_one - pays attention to the corresponding token\n",
    "    next_token - pays attention to the next token\n",
    "    prev_token - pays attention to the previous token\n",
    "    cls_token  - pays attention to the first index ([CLS])\n",
    "    '''\n",
    "\n",
    "    one_to_one = torch.eye(attentions[0].shape[-1])\n",
    "    next_token = torch.cat((torch.cat((torch.zeros(attentions[0].shape[-1]-1, 1), torch.eye(attentions[0].shape[-1]-1)), dim=1),\\\n",
    "        torch.zeros(1, attentions[0].shape[-1])), dim=0)\n",
    "    prev_token = torch.cat((torch.zeros(1, attentions[0].shape[-1]), \\\n",
    "        torch.cat((torch.eye(attentions[0].shape[-1]-1), torch.zeros(attentions[0].shape[-1]-1, 1)), dim=1)), dim=0)\n",
    "    cls_token = torch.zeros(attentions[0].shape[-1], attentions[0].shape[-1])\n",
    "    cls_token[:,0] = 1.\n",
    "\n",
    "    return [one_to_one, next_token, prev_token, cls_token]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ag_loss(inputs, attentions, device, attn_head_types='0,1,1,4'):\n",
    "        '''\n",
    "        Adds a random loss based on attention values\n",
    "        To test gradients\n",
    "        outputs[-1] contains the attention values (tuple of size num_layers)\n",
    "        and each elements is of the shape\n",
    "        [batch_size X num_heads X max_sequence_len X max_sequence_len]\n",
    "        '''\n",
    "        # Get the attention head types\n",
    "        attn_head_types = [int(i) for i in attn_head_types.split(',')]\n",
    "\n",
    "        # The number attention heads of each type. one-to-one, next, previous, first\n",
    "        numbers = attn_head_types\n",
    "        print('numbers:', numbers)\n",
    "        cum_sum = np.cumsum(numbers)\n",
    "        print('cum_sum:', cum_sum)\n",
    "        # Matrices containing the attention patterns\n",
    "        targets = create_pos_attn_patterns(attentions)\n",
    "        # Loss for positional attention patterns\n",
    "        expanded_targets = []\n",
    "        loss = torch.nn.MSELoss()\n",
    "        total_loss = 0.        \n",
    "        # Change the tensor's dimension\n",
    "        for (num, target) in zip(numbers, targets):\n",
    "            if num == 0:\n",
    "                expanded_targets.append(None)\n",
    "            else:\n",
    "                # Add dimensions so that the tensor can be repeated\n",
    "                target = torch.unsqueeze(target, 0)\n",
    "                target = torch.unsqueeze(target, 0)\n",
    "                # Change the target tensor's dimension so that it matches batch_size X num_heads[chosen]\n",
    "                target = target.repeat(attentions[0].shape[0], num, 1, 1)\n",
    "                target = target.to(device)\n",
    "                expanded_targets.append(target)\n",
    "\n",
    "        # Go over all the layers\n",
    "        for i in range(len(attentions)):\n",
    "            for j in range(len(numbers)):\n",
    "                if expanded_targets[j] is not None:\n",
    "                    if j == 0:\n",
    "                        total_loss += loss(expanded_targets[j], attentions[i][:,0:cum_sum[j]])\n",
    "                    else:\n",
    "                        total_loss += loss(expanded_targets[j], attentions[i][:,cum_sum[j-1]:cum_sum[j]])\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numbers: [0, 1, 1, 4]\n",
      "cum_sum: [0 1 2 6]\n"
     ]
    }
   ],
   "source": [
    "scale = 1\n",
    "mlm_loss = output[0]\n",
    "ag_loss = compute_ag_loss(source_ids, \n",
    "                            output[2].attentions, \n",
    "                            device, attn_head_types='0,1,1,4')\n",
    "\n",
    "# loss = mlm_loss + ag_loss * scale * linear_schedule_for_scale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlm_loss: 0.772885262966156 ag_loss: 0.09443830698728561 total_loss: 0.8673235699534416\n"
     ]
    }
   ],
   "source": [
    "print('mlm_loss:', mlm_loss.item(), 'ag_loss:', ag_loss.item(), 'total_loss:', mlm_loss.item() + ag_loss.item() * scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuBERT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8492d8711f92acbd6f1f5de70669e0e0dfa9ae8288673c2cd77f66771fab39c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
